{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"Fig/UGA.png\" width=\"30%\" height=\"30%\"></center>\n",
    "<center><h3>Master of Science in Industrial and Applied Mathematics (MSIAM)  - 1st year</h3></center>\n",
    "<hr>\n",
    "<center><h1>Numerical Optimization</h1></center>\n",
    "<center><h2>Lab 5: Optimization for ML </h2></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "import logistic_regression_ionosphere as pb\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Algorithms performance on practical problems\n",
    "\n",
    "In this lab, we will investigate how to evaluate and display performance of optimization algorithms over a practical problem of machine learning: binary classification using logistic regression.</br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning as an Optimization problem\n",
    "\n",
    "We have some *data*  $\\mathcal{D}$ consisting of $m$ *examples* $\\{d_i\\}$; each example consisting of a *feature* vector $a_i\\in\\mathbb{R}^d$ and an *observation* $b_i\\in \\mathcal{O}$: $\\mathcal{D} = \\{[a_i,b_i]\\}_{i=1..m}$. In this lab, we will consider the <a href=\"https://archive.ics.uci.edu/ml/machine-learning-databases/ionosphere/ionosphere.names\">ionosphere</a> dataset.\n",
    " \n",
    "The goal of *supervised learning* is to construct a predictor for the observations when given feature vectors.\n",
    "\n",
    "A popular approach is based on *linear models* which are based on finding a *parameter* $x$ such that the real number $\\langle a_i , x \\rangle$ is used to predict the value of the observation through a *predictor function* $g:\\mathbb{R}\\to \\mathcal{O}$: $g(\\langle a_i , x \\rangle)$ is the predicted value from $a_i$.\n",
    "\n",
    "In order to find such a parameter, we use the available data and a *loss* $\\ell$ that penalizes the error made between the predicted $g(\\langle a_i , x \\rangle)$ and observed $b_i$ values. For each example $i$, the corresponding error function for a parameter $x$ is $f_i(x) =   \\ell( g(\\langle a_i , x \\rangle) ; b_i )$. Using the whole data, the parameter that minimizes the total error is the solution of the minimization problem\n",
    "\n",
    "$$ \\min_{x\\in\\mathbb{R}^d}  \\frac{1}{m} \\sum_{i=1}^m f_i(x) = \\frac{1}{m} \\sum_{i=1}^m  \\ell( g(\\langle a_i , x \\rangle) ; b_i ). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binary Classification with Logisitic Regression\n",
    "\n",
    "In our setup, the observations are binary: $\\mathcal{O} = \\{-1 , +1 \\}$, and the *Logistic loss* is used to form the following optimization problem\n",
    "\\begin{align*}\n",
    "\\min_{x\\in\\mathbb{R}^d } f(x) := \\frac{1}{m}  \\sum_{i=1}^m  \\log( 1+\\exp(-b_i \\langle a_i,x \\rangle) ) + \\frac{\\lambda_2}{2} \\|x\\|_2^2.\n",
    "\\end{align*}\n",
    "where the last term is added as a regularization (of type $\\ell_2$, aka Tikhnov) to prevent overfitting.\n",
    "\n",
    "Under some statistical hypotheses, $x^\\star = \\arg\\min f(x)$ maximizes the likelihood of the labels knowing the features vector. Then, for a new point $d$ with features vector $a$, \n",
    "$$ p_1(a) = \\mathbb{P}[d\\in \\text{ class }  +1] = \\frac{1}{1+\\exp(-\\langle a;x^\\star \\rangle)} $$\n",
    "\n",
    "Thus, from $a$, if $p_1(a)$ is close to $1$, one can decide that $d$ belongs to class $1$; and the opposite decision if $p(a)$ is close to $0$. Between the two, the appreciation is left to the data scientist depending on the application.\n",
    "\n",
    "## Objective of the optimizer\n",
    " \n",
    "Given oracles for the function and its gradient, as well as an upper-bound of the Lipschitz constant $L$ of the gradient, find a minimizer of $f$.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> You are given *all* oracles of $f$ (function, gradient, Hessian) in `logistic_regression_ionosphere.py` and several algorithms in `algoGradient.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Influence of strong convexity on the speed of the gradient method\n",
    "\n",
    "\n",
    "> **Task 1**: Run the following  blocks for different values of parameter `lam2` of the problem. What do you notice in terms of speed of convergence, what is the reason?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algoGradient_corr import gradient_algorithm\n",
    "\n",
    "#### Parameter we give at our algorithm\n",
    "PREC    = 1e-5                     # Sought precision\n",
    "ITE_MAX = 5000                     # Max number of iterations\n",
    "x0      = np.zeros(pb.n)           # Initial point\n",
    "step    = 1.0/pb.L\n",
    "\n",
    "pb.lam2 = 0.1\n",
    "\n",
    "##### gradient algorithm\n",
    "x,x_tab = gradient_algorithm(pb.f , pb.f_grad , x0 , step , PREC , ITE_MAX )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = range(x_tab.shape[0])\n",
    "ys = [ pb.f(x_tab[ind]) for ind in xs ]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xs, ys, linewidth=1.0, linestyle='-', label='gradient', color='black')\n",
    "ax.set(xlabel='iteration k', ylabel='f(x_k)')\n",
    "ax.legend(); ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accelerating poorly conditioned problems\n",
    "\n",
    "While the addition of strong convexity accelerates the rate in practice, it usually result shift the solutions of the original problem. For a learning problem, it affects the accuracy.\n",
    "\n",
    "In order to get faster convergences when the rate is slower, several acceleration techniques exist. We are going to present the most common in the following.\n",
    "\n",
    "### Nesterov's fast gradient\n",
    "\n",
    "In a series of papers published in the 80's, Yu. Nesterov proposed an acceleration technique in order to make the worst case rate of the gradient algorithm from $\\mathcal{O}(1/k)$ to  $\\mathcal{O}(1/k^2)$. This technique is now immensely popular, notably in the machine learning and image processing communities.\n",
    " \n",
    "\n",
    "The iterations of Nesterov's accelerated gradient are as such:\n",
    "$$ \\left\\{  \\begin{array}{ll}  x_{k+1} = y_k - \\gamma \\nabla f(y_k) \\\\ y_{k+1} = x_{k+1} + \\alpha_{k+1} (x_{k+1} - x_k )  \\end{array}           \\right. $$\n",
    "with $\\alpha_{k+1} = \\frac{k+2}{k+3}$.\n",
    " \n",
    "Although no clear intuition can be drawn, the extended point $y_{k+1}$ can be seen as an extension by inertia of the last points $x_{k+1}$, $x_k$.\n",
    "\n",
    "*Note* that the original paper from Nesterov suggested more elaborate intertial parameters defined as\n",
    "$$ \\alpha_{k+1} = \\frac{\\lambda_k -1 }{\\lambda_{k+1}} \\text{ with } \\lambda_0 = 0 \\text{ and } \\lambda_{k+1} = \\frac{1+\\sqrt{1+4\\lambda_k^2}}{2} . $$\n",
    "Those are \"optimal\" in some sense, but the above simpler ones provide the $\\mathcal O(1/k^2)$ as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 2**: Implement Nesterov's fast gradient algorithm in function `fast_gradient_algorithm` of file `algoGradient.py`.\n",
    "\n",
    "> **Task 3**: Run the constant stepsize and fast gradient algorithms and compare the convergence rates (for lam2 = 0.001)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algoGradient_corr import fast_gradient_algorithm\n",
    "\n",
    "#### Parameter we give at our algorithm\n",
    "PREC    = 1e-5                     # Sought precision\n",
    "ITE_MAX = 50                     # Max number of iterations\n",
    "x0      = np.zeros(pb.n)           # Initial point\n",
    "step    = 1.0/pb.L\n",
    "\n",
    "pb.lam2 = 0.001\n",
    "\n",
    "x,x_tab    =      gradient_algorithm(pb.f , pb.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "xF,xF_tab  = fast_gradient_algorithm(pb.f , pb.f_grad , x0 , step , PREC , ITE_MAX )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_grad = range(x_tab.shape[0])\n",
    "xs_fastgrad = range(xF_tab.shape[0])\n",
    "\n",
    "## Plot function values\n",
    "ys_f_grad = [pb.f(x_tab[i]) for i in xs_grad]\n",
    "ys_f_fastgrad = [pb.f(xF_tab[i]) for i in xs_fastgrad]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xs_grad, ys_f_grad, color=\"black\", linewidth=1.0, linestyle=\"-\",label='gradient')\n",
    "ax.plot(xs_fastgrad, ys_f_fastgrad, color=\"red\", linewidth=1.0, linestyle=\"-\",label='fast gradient')\n",
    "ax.set(xlabel='iteration k', ylabel='f(x_k)')\n",
    "ax.legend(); ax.grid()\n",
    "\n",
    "## Plot gradient norm\n",
    "ys_normgrad_grad = [np.linalg.norm(pb.f_grad(x_tab[i])) for i in xs_grad]\n",
    "ys_normgrad_fastgrad = [np.linalg.norm(pb.f_grad(xF_tab[i])) for i in xs_fastgrad]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(xs_grad, ys_normgrad_grad, color=\"black\", linewidth=1.0, linestyle=\"-\",label='gradient')\n",
    "ax.plot(xs_fastgrad, ys_normgrad_fastgrad, color=\"red\", linewidth=1.0, linestyle=\"-\",label='fast gradient')\n",
    "ax.set(xlabel='iteration k', ylabel='||∇f(x_k)||')\n",
    "ax.legend(); ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### Other methods: line-search, BFGS\n",
    "\n",
    "\n",
    "Other popular methods to accelerate convergence are:\n",
    "* line-search: given a point $x_k$ and a (descent) direction $d_k$, the linesearch computes a *steplength* $\\alpha_k$ and yields the following next iterate $x_k+\\alpha_k d_k$. The steplength $\\alpha_k$ is chosen such that $f(x_k+\\alpha_k d_k)$ is sufficiently lower than $f(x_k)$ (*sufficient decrease* condition).\n",
    "* BFGS which is a Quasi-Newton method in the sense that it approximates second order information in an online setting.\n",
    "\n",
    "Wolfe's linesearch is implemented in `Scipy`'s <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.line_search.html\">`scipy.optimize.line_search`</a>. \n",
    "\n",
    "> **Task 4**: Implement the gradient algorithm with Wolfe's line-search in function `gradient_wolfe` of file `algoGradient.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BFGS.** (Broyden-Fletcher-Goldfarb-Shanno, 1970) The popular BFGS algorithm consist in performing the following iteration\n",
    "$$ x_{k+1}=x_k - \\gamma_k W_k \\nabla f(x_k)$$\n",
    "where $\\gamma_k$ is given by Wolfe's line-search and positive definite matrix $W_k$ is computed as\n",
    "$$ W_{k+1}=W_k - \\frac{s_k y_k^T W_k+W_k y_k s_k^T}{y_k^T s_k} +\\left[1+\\frac{y_k^T W_k y_k}{y_k^T s_k}\\right]\\frac{s_k s_k^T}{y_k^T s_k} $$\n",
    "with $s_k=x_{k+1}-x_{k}$ and $y_k=\\nabla f(x_{k+1}) - \\nabla f(x_{k})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Task 5**: Implement BFGS in function `bfgs` of  file `algoGradient.py`.\n",
    "\n",
    "*Note*: Linear algebra in numpy can be tricky. Here is a quick reference of usefull methods, $y$, $s$ denote vectors, $A$, $B$ matrices:\n",
    "\n",
    "| Math expression | Numpy method |\n",
    "| --- | --- |\n",
    "| $y^T s$ | `np.dot(y, s)` |\n",
    "| $y s^T$ | `np.outer(y, s)` |\n",
    "| $A B$ | `np.matmul(A, B)` |\n",
    "\n",
    "> **Task 6**: Compare the performance of the previously investigated algorithms. *(Note that you can also test the performance of Newton's method although it is a bit unfair compared to the other algorithms as the variable size is small)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algoGradient_corr import gradient_Wolfe, bfgs\n",
    "\n",
    "#### Parameter we give our algorithms\n",
    "PREC    = 1e-13                     # Sought precision\n",
    "ITE_MAX = 500                      # Max number of iterations\n",
    "x0      = np.zeros(pb.n)           # Initial point\n",
    "step    = 1.0/pb.L\n",
    "\n",
    "##### gradient algorithm\n",
    "x,x_tab = gradient_algorithm(pb.f , pb.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n",
    "##### fast gradient algorithm\n",
    "xF,xF_tab  = fast_gradient_algorithm(pb.f , pb.f_grad , x0 , step , PREC , ITE_MAX )\n",
    "\n",
    "##### Wolfe line-search algorithm\n",
    "xW,xW_tab = gradient_Wolfe(pb.f , pb.f_grad , x0 , PREC , ITE_MAX )\n",
    "\n",
    "##### BFGS algorithm\n",
    "xB,xB_tab = bfgs(pb.f , pb.f_grad , x0 , PREC , ITE_MAX )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = [pb.f(x_tab[i]) for i in range(x_tab.shape[0])]\n",
    "G = [np.linalg.norm(pb.f_grad(x_tab[i] )) for i in range(x_tab.shape[0])]\n",
    "\n",
    "FF = [pb.f(xF_tab[i]) for i in range(xF_tab.shape[0])]\n",
    "GF = [np.linalg.norm(pb.f_grad(xF_tab[i] )) for i in range(xF_tab.shape[0])]\n",
    "\n",
    "FW = [pb.f(xW_tab[i]) for i in range(xW_tab.shape[0])]\n",
    "GW = [np.linalg.norm(pb.f_grad(xW_tab[i] )) for i in range(xW_tab.shape[0])]\n",
    "\n",
    "FB = [pb.f(xB_tab[i]) for i in range(xB_tab.shape[0])]\n",
    "GB = [np.linalg.norm(pb.f_grad(xB_tab[i] )) for i in range(xB_tab.shape[0])]\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot( F, color=\"black\", linewidth=1.0, linestyle=\"-\",label='gradient')\n",
    "plt.plot( FF, color=\"red\", linewidth=1.0, linestyle=\"-\",label='fast gradient')\n",
    "plt.plot( FW, color=\"magenta\", linewidth=1.0, linestyle=\"-\",label='Wolfe')\n",
    "plt.plot( FB, color=\"green\", linewidth=1.0, linestyle=\"-\",label='BFGS')\n",
    "plt.grid(True); plt.legend(); plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.plot( G, color=\"black\", linewidth=1.0, linestyle=\"-\",label='gradient')\n",
    "plt.plot( GF, color=\"red\", linewidth=1.0, linestyle=\"-\",label='fast gradient')\n",
    "plt.plot( GW, color=\"magenta\", linewidth=1.0, linestyle=\"-\",label='Wolfe')\n",
    "plt.plot( GB, color=\"green\", linewidth=1.0, linestyle=\"-\",label='BFGS')\n",
    "plt.yscale('log'); plt.xscale('log'); \n",
    "plt.grid(True); plt.legend(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Performance on learning problems\n",
    "\n",
    "### Prediction power\n",
    "\n",
    "\n",
    "\n",
    "Our problem of interest is binary classification using logistic regression.</br>\n",
    "Although this is a machine learning task, the predictor construction amounts to minimizing a smooth convex optimization function $f$ called the *loss*, the final minimizer is called a *predictor* and its scalar product with the data vector gives a probability of belonging to class $1$.\n",
    "\n",
    "The previous test was based on the functional decrease whereas our task is binary classification. Let us look at the final accuracies obtained.\n",
    "\n",
    "> **Task 7**: The file `logistic_regression.py` contains a `prediction` function that takes a *predictor* and resturn the accuracy of the predictor. Take a look at how the function is defined.\n",
    "\n",
    "> **Task 8**: Observe the accuracy of all final points obtained before. What do you notice? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred,perf = pb.prediction(x,PRINT=False)\n",
    "print(\"Gradient algorithm: \\t{:.2f}%\".format(perf*100))\n",
    "\n",
    "predF,perfF = pb.prediction(xF,PRINT=False)\n",
    "print(\"Fast Gradient: \\t\\t{:.2f}%\".format(perfF*100))\n",
    "\n",
    "predW,perfW = pb.prediction(xW,PRINT=False)\n",
    "print(\"Wolfe: \\t\\t\\t{:.2f}%\".format(perfW*100))\n",
    "\n",
    "predB,perfB = pb.prediction(xB,PRINT=False)\n",
    "print(\"BFGS: \\t\\t\\t{:.2f}%\".format(perfB*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predF,perfF = pb.prediction(xF,PRINT=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Going further\n",
    "\n",
    "Let's return to gradient-based methods for minimizing a smooth convex function (*i.e.* differentiable with Lipschitz gradient). \n",
    "\n",
    "> **Task 9**: Can you think of an *adaptive stepsize* that leverages the assumption on the gradient? \n",
    "\n",
    "You may want to return to the proof of convergence of the fixed stepsize gradient method, and find where the limitation on the available stepsize appear in the proof of convergence."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
